import streamlit as st
from PIL import Image
import torch
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
import json

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
IMG_SIZE = (192, 256)

# 1. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
# === –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ===
def resize_and_center(img, size):
    """
    –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —á–µ—Ä–Ω—ã–µ –ø–æ–ª—è
    –¥–ª—è —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞
    """
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é, —á—Ç–æ–±—ã –Ω–µ –∏–∑–º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª
    img = img.copy()

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—é (–∞–ª—å–±–æ–º–Ω–∞—è/–ø–æ—Ä—Ç—Ä–µ—Ç–Ω–∞—è)
    is_landscape = img.width > img.height

    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –ø–æ –±–æ–ª—å—à–µ–π —Å—Ç–æ—Ä–æ–Ω–µ
    if is_landscape:
        new_width = size[0]
        new_height = int(size[0] * img.height / img.width)
    else:
        new_height = size[1]
        new_width = int(size[1] * img.width / img.height)

    # –†–µ—Å–∞–π–∑ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π
    img = img.resize((new_width, new_height), Image.BILINEAR)

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ö–æ–ª—Å—Ç —Å —á–µ—Ä–Ω—ã–º —Ñ–æ–Ω–æ–º
    new_img = Image.new("RGB", size, (0, 0, 0))

    # –¶–µ–Ω—Ç—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    left = (size[0] - img.width) // 2
    top = (size[1] - img.height) // 2
    new_img.paste(img, (left, top))

    return new_img


# === –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ===
transform = transforms.Compose([
    transforms.Lambda(lambda img: resize_and_center(img, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)
])


# === –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö ===
def load_metadata():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ JSON-—Ñ–∞–π–ª–æ–≤"""
    try:
        with open('class_names.json', 'r', encoding='utf-8') as f:
            class_names = json.load(f)

        with open('class_descriptions.json', 'r', encoding='utf-8') as f:
            class_descriptions = json.load(f)

        return class_names, class_descriptions
    except FileNotFoundError:
        st.error("–û—à–∏–±–∫–∞: —Ñ–∞–π–ª—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        st.stop()


CLASS_NAMES, CLASS_DESCRIPTIONS = load_metadata()

@st.cache_resource
def load_model():
    model = torch.jit.load("trashnet_mobile.pt", map_location='cpu')
    model.eval()
    return model

model = load_model()

# === –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ===
st.title("üóëÔ∏è –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –º—É—Å–æ—Ä–∞ –ø–æ —Ñ–æ—Ç–æ")
uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º—É—Å–æ—Ä–∞", type=["jpg", "jpeg", "png"])

if uploaded_file:
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", use_container_width=True)


    input_tensor = transform(image).unsqueeze(0)

    with torch.no_grad():
        output = model(input_tensor)
        pred_index = output.argmax(1).item()
        pred_class = CLASS_NAMES[pred_index]
        description = CLASS_DESCRIPTIONS.get(pred_class, pred_class)  # –µ—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ—Ç ‚Äî –≤—ã–≤–æ–¥–∏–º –ø—Ä–æ—Å—Ç–æ –º–µ—Ç–∫—É
        st.success(f"üß† –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: **{description}**")
